#!/usr/bin/env python3
"""
Apply harvested works for zero-coverage thinkers into the data-v2 dataset.

The script reads per-thinker harvest JSON files generated by
harvest_zero_work_thinkers.py and populates the corresponding entries under
public/data-v2/<collection>/<Thinker>/<Subject>.json. Metadata entries are
updated with refreshed work counts and subject summaries.

Usage:
    python scripts/python/scrapers/apply_zero_works_harvest.py \
        --harvest-dir data/zero-works-harvest \
        --data-dir public/data-v2
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Iterable, List, Tuple


DEFAULT_SUBJECT = "General"


def load_harvest_records(harvest_dir: Path) -> Iterable[Tuple[Path, dict]]:
    """Yield (path, payload) pairs for each harvest JSON file."""
    for file_path in sorted(harvest_dir.rglob("*.json")):
        try:
            payload = json.loads(file_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as exc:
            print(f"[ERROR] Failed to parse {file_path}: {exc}")
            continue
        yield file_path, payload


def ensure_thinker_directory(base_dir: Path, collection: str, thinker: str) -> Path:
    """Create the thinker directory if it does not exist and return its path."""
    thinker_dir = base_dir / collection / thinker
    thinker_dir.mkdir(parents=True, exist_ok=True)
    return thinker_dir


def load_metadata(collection_dir: Path) -> List[Dict[str, object]]:
    metadata_file = collection_dir / "metadata.json"
    if not metadata_file.exists():
        raise FileNotFoundError(f"Missing metadata.json for collection: {collection_dir}")
    return json.loads(metadata_file.read_text(encoding="utf-8"))


def save_metadata(collection_dir: Path, metadata: List[Dict[str, object]]) -> None:
    metadata_file = collection_dir / "metadata.json"
    metadata_file.write_text(json.dumps(metadata, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")


def update_metadata_entry(entry: Dict[str, object], subject: str, work_count: int) -> None:
    """Set overall work count and per-subject tally on a metadata entry."""
    entry["w"] = work_count

    subjects = entry.get("subjects") or []
    updated = False
    for subject_entry in subjects:
        if subject_entry.get("name") == subject:
            subject_entry["count"] = work_count
            updated = True
            break

    if not updated:
        subjects.append({"name": subject, "count": work_count})

    entry["subjects"] = subjects


def apply_harvest_record(
    base_dir: Path,
    collection: str,
    thinker: str,
    works: List[Dict[str, str]],
    subject: str = DEFAULT_SUBJECT,
) -> None:
    thinker_dir = ensure_thinker_directory(base_dir, collection, thinker)
    subject_file = thinker_dir / f"{subject}.json"

    unique_by_url: Dict[str, Dict[str, str]] = {}
    for item in works:
        url = item.get("url")
        title = item.get("title")
        if not url or not title:
            continue
        unique_by_url.setdefault(url, {"title": title.strip(), "url": url})

    # Sort works by title for determinism
    sorted_works = sorted(unique_by_url.values(), key=lambda item: item["title"].lower())
    subject_file.write_text(json.dumps(sorted_works, indent=2, ensure_ascii=False) + "\n", encoding="utf-8")

    metadata = load_metadata(base_dir / collection)
    for entry in metadata:
        if entry.get("n") == thinker:
            update_metadata_entry(entry, subject, len(sorted_works))
            break
    else:
        print(f"[WARN] Metadata entry not found for thinker '{thinker}' in collection '{collection}'.")
        return

    save_metadata(base_dir / collection, metadata)


def main() -> None:
    parser = argparse.ArgumentParser(description="Apply harvested works into data-v2 structure.")
    parser.add_argument(
        "--harvest-dir",
        type=Path,
        default=Path("data/zero-works-harvest"),
        help="Directory containing per-thinker harvest JSON files.",
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("public/data-v2"),
        help="Root directory of the structured dataset.",
    )
    parser.add_argument(
        "--subject",
        type=str,
        default=DEFAULT_SUBJECT,
        help="Subject label to use when writing works (default: General).",
    )
    args = parser.parse_args()

    if not args.harvest_dir.exists():
        raise FileNotFoundError(f"Harvest directory not found: {args.harvest_dir}")
    if not args.data_dir.exists():
        raise FileNotFoundError(f"Data directory not found: {args.data_dir}")

    applied = 0
    skipped = 0
    failed = 0

    for file_path, payload in load_harvest_records(args.harvest_dir):
        status = payload.get("status")
        collection = payload.get("collection")
        thinker = payload.get("thinker")
        works = payload.get("works", [])

        if status != "success":
            skipped += 1
            continue

        if not collection or not thinker:
            print(f"[WARN] Missing collection or thinker in {file_path}, skipping.")
            failed += 1
            continue

        if not works:
            print(f"[WARN] No works in successful harvest for {thinker}, skipping.")
            failed += 1
            continue

        try:
            apply_harvest_record(args.data_dir, collection, thinker, works, subject=args.subject)
            applied += 1
        except FileNotFoundError as exc:
            print(f"[ERROR] {exc}")
            failed += 1
        except OSError as exc:
            print(f"[ERROR] Failed to write files for {thinker}: {exc}")
            failed += 1

    print(
        f"Applied harvest for {applied} thinkers. "
        f"Skipped (non-success): {skipped}. Failures: {failed}."
    )


if __name__ == "__main__":
    main()


